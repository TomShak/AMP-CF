{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d61-vtVavznp"
   },
   "source": [
    "# Imports \\ Device \\ Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "r8LGY0KnvwOz",
    "outputId": "b3343910-d464-461a-d13a-6997bc07ad24"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################### Imports ####################################\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "from tabulate import tabulate\n",
    "from random import shuffle\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "########################## Device #####################################\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(0)\n",
    "########################### Seed ######################################\n",
    "RND_SEED = 123\n",
    "SEED99 = 123\n",
    "random.seed(RND_SEED)\n",
    "torch.manual_seed(RND_SEED)\n",
    "RANDOM_STATE = RND_SEED\n",
    "np.random.seed(RND_SEED)\n",
    "########################### Display ####################################\n",
    "from IPython.core.display import display, HTML\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrStJudpv7D2"
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EiIZyXyvwe0"
   },
   "outputs": [],
   "source": [
    "def try_download(url, download_path):\n",
    "    archive_name = url.split('/')[-1]\n",
    "    folder_name, _ = os.path.splitext(archive_name)\n",
    "\n",
    "    try:\n",
    "        r = urlopen(url)\n",
    "    except URLError as e:\n",
    "        print('Cannot download the data. Error: %s' % s)\n",
    "        return\n",
    "\n",
    "    assert r.status == 200\n",
    "    data = r.read()\n",
    "\n",
    "    with zipfile.ZipFile(io.BytesIO(data)) as arch:\n",
    "        arch.extractall(download_path)\n",
    "\n",
    "    print('The archive is extracted into folder: %s' % download_path)\n",
    "    \n",
    "#################################################################################\n",
    "\n",
    "def read_data(path):\n",
    "    files = {}\n",
    "    for filename in path.glob('*'):\n",
    "        if filename.suffix == '.csv':\n",
    "            files[filename.stem] = pd.read_csv(filename)\n",
    "        elif filename.suffix == '.dat':\n",
    "            if filename.stem == 'ratings':\n",
    "                columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "            else:\n",
    "                columns = ['movieId', 'title', 'genres']\n",
    "            data = pd.read_csv(filename, sep='::', names=columns, engine='python')\n",
    "            files[filename.stem] = data\n",
    "    return files['ratings'], files['movies']\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def create_dataset(ratings, top=None):\n",
    "    if top is not None:\n",
    "        ratings.groupby('userId')['rating'].count()\n",
    "\n",
    "    unique_users = ratings.userId.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.userId.map(user_to_index)\n",
    "\n",
    "    unique_movies = ratings.movieId.unique()\n",
    "    movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "    new_movies = ratings.movieId.map(movie_to_index)\n",
    "\n",
    "    index_to_movieId = dict(map(reversed, movie_to_index.items()))\n",
    "    index_to_userId = dict(map(reversed, user_to_index.items()))\n",
    "\n",
    "    n_users = unique_users.shape[0]\n",
    "    n_movies = unique_movies.shape[0]\n",
    "\n",
    "    X = pd.DataFrame({'userId': new_users, 'movieId': new_movies})\n",
    "    y = ratings['rating'].astype(np.float32)\n",
    "    t = ratings['timestamp'].astype(np.float64)\n",
    "\n",
    "    return (n_users, n_movies), (X, y, t), (user_to_index, movie_to_index), (index_to_movieId, index_to_userId)\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def binarize(y, ds): # binarize dataset\n",
    "    if ds == 'movielens':\n",
    "      y_binary = deepcopy(y)\n",
    "      y_binary[y > -1] = 1\n",
    "    if ds == 'amazon':\n",
    "      y_binary = deepcopy(y)\n",
    "      y_binary[y < 4] = 0\n",
    "      y_binary[y >= 4] = 1\n",
    "    if ds == 'yahoo':\n",
    "      y_binary = deepcopy(y)\n",
    "      y_binary[y < 85] = 0\n",
    "      y_binary[y == 255] = 0\n",
    "      y_binary[y >= 85] = 1\n",
    "    return y_binary\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "class ReviewsIterator:\n",
    "\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "\n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k * bs:(k + 1) * bs], self.y[k * bs:(k + 1) * bs]\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in ReviewsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1)\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def create_user_movie_matrix(X, y, n, m):\n",
    "    ratings_mat = np.zeros([n, m])\n",
    "    for i, sample in enumerate(X.values):\n",
    "        user_idx = sample[0]\n",
    "        movie_idx = sample[1]\n",
    "        ratings_mat[user_idx][movie_idx] = y.values[i]\n",
    "    return ratings_mat\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def filter_dataset(ratings):\n",
    "    \n",
    "    ##### filtering users with less then 2 interactions #####\n",
    "    print('Filtering Data')\n",
    "    \n",
    "    (n, m), (X, y_binary,\n",
    "             t), (user_to_index, movie_to_index), (index_to_movieId,\n",
    "                                                   index_to_userId) = create_dataset(ratings)\n",
    "\n",
    "    ratings_mat_full = create_user_movie_matrix(X, y_binary, n, m)\n",
    "    pos_samples_idx_mat = np.argwhere(ratings_mat_full == 1).tolist()\n",
    "\n",
    "    pos_samples_idx_dict = {}\n",
    "    for i in range(n):\n",
    "        pos_samples_idx_dict[i] = []\n",
    "    for pos_sample_id in pos_samples_idx_mat:\n",
    "        pos_samples_idx_dict[pos_sample_id[0]].append(pos_sample_id[1])\n",
    "\n",
    "    userIds_to_delete = []\n",
    "    for idx in pos_samples_idx_dict:\n",
    "        if len(pos_samples_idx_dict[idx]) < 2:\n",
    "            userId = index_to_userId[idx]\n",
    "            userIds_to_delete.append(userId)\n",
    "\n",
    "    ratings.set_index('userId', inplace=True)\n",
    "    ratings.drop(index=userIds_to_delete, inplace=True)\n",
    "    ratings.reset_index(inplace=True)\n",
    "\n",
    "    ##### filtering movies with less then 2 interactions #####\n",
    "    (n, m), (X, y_binary, t), (user_to_index, movie_to_index), (index_to_movieId, index_to_userId) = create_dataset(\n",
    "        ratings)\n",
    "\n",
    "    ratings_mat_full = create_user_movie_matrix(X, y_binary, n, m)\n",
    "    pos_samples_idx_mat = np.argwhere(ratings_mat_full == 1).tolist()\n",
    "\n",
    "    pos_samples_idx_dict = {}\n",
    "    for i in range(m):\n",
    "        pos_samples_idx_dict[i] = []\n",
    "    for pos_sample_id in pos_samples_idx_mat:\n",
    "        pos_samples_idx_dict[pos_sample_id[1]].append(pos_sample_id[0])\n",
    "\n",
    "    movieIds_to_delete = []\n",
    "    for idx in pos_samples_idx_dict:\n",
    "        if len(pos_samples_idx_dict[idx]) < 2:\n",
    "            movieId = index_to_movieId[idx]\n",
    "            movieIds_to_delete.append(movieId)\n",
    "\n",
    "    ratings.set_index('movieId', inplace=True)\n",
    "    ratings.drop(index=movieIds_to_delete, inplace=True)\n",
    "    ratings.reset_index(inplace=True)\n",
    "    ratings.set_index('userId', inplace=True)\n",
    "    ratings.reset_index(inplace=True)\n",
    "    return ratings\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def prep_batches(datasets, bs):\n",
    "    batches_dict = {'train': {'x_batches': [], 'y_batches': []}}\n",
    "    for batch in batches(*datasets['train'], shuffle=True, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        x_batch = x_batch.cpu().numpy()\n",
    "        y_batch = y_batch.cpu().numpy()\n",
    "        batches_dict['train']['x_batches'].append(x_batch)\n",
    "        batches_dict['train']['y_batches'].append(y_batch)\n",
    "\n",
    "    print(\"# of training batches: \", len(batches_dict['train']['x_batches']))\n",
    "    return batches_dict\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def create_test_dict(ratings):\n",
    "    (n, m), (X, y_binary, t), (user_to_index, movie_to_index), (index_to_movieId, index_to_userId) = create_dataset(\n",
    "        ratings)\n",
    "    \n",
    "    ratings_mat_full = create_user_movie_matrix(X, y_binary, n, m)\n",
    "    pos_samples_idx_mat = np.argwhere(ratings_mat_full == 1).tolist()\n",
    "    \n",
    "    pos_samples_idx_dict = {}\n",
    "    for i in range(n):\n",
    "        pos_samples_idx_dict[i] = []\n",
    "    for pos_sample_idx in pos_samples_idx_mat:\n",
    "        pos_samples_idx_dict[pos_sample_idx[0]].append(pos_sample_idx[1])\n",
    "\n",
    "    df = pd.DataFrame(index=[ratings.userId.to_list(), ratings.movieId.to_list()],\n",
    "                      columns=['rating', 'timestamp'])\n",
    "    df.index.names = ['userId', 'movieId']\n",
    "    df.rating = ratings.rating.values\n",
    "    df.timestamp = ratings.timestamp.values\n",
    "\n",
    "    '''for every user idx, pick a movie idx, add as key:value pair to test dict\n",
    "    use multiindexing on the ratings dataframe, then drop the relevent rows'''\n",
    "    test_dict = {}\n",
    "    ratingIds_to_delete = []\n",
    "    random.seed(RND_SEED)\n",
    "    \n",
    "    for user_idx in pos_samples_idx_dict:\n",
    "        userId = index_to_userId[user_idx]\n",
    "        movie_idxs = random.sample(pos_samples_idx_dict[user_idx], k = 1)\n",
    "        test_dict[userId] = []\n",
    "        movieId = index_to_movieId[movie_idxs[0]]\n",
    "        test_dict[userId].append(movieId)\n",
    "        ratingIds_to_delete.append((userId, movieId))\n",
    "    \n",
    "    df.drop(index=ratingIds_to_delete, inplace=True)\n",
    "    df.reset_index(level=[0, 1], inplace=True)\n",
    "    \n",
    "    print(\"Amount of training samples: \", df.shape[0])\n",
    "    return df, test_dict\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def prep_trial(ds):\n",
    "  \n",
    "    movies = None\n",
    "          \n",
    "    if ds == 'yahoo_music_data':\n",
    "        ratings = pd.read_csv('./Datasets/{0}.csv'.format(ds), header=None, names=['userId', 'movieId', 'rating', 'timestamp'],  engine='python')\n",
    "        ds_ = 'yahoo'\n",
    "        \n",
    "    if ds == 'amazon_Video_Games_data':\n",
    "        ratings = pd.read_csv('./Datasets/{0}.csv'.format(ds), header=None, names=['userId', 'movieId', 'rating', 'timestamp'],  engine='python')\n",
    "        ds_ = 'amazon'\n",
    "      \n",
    "    if ds == 'ml-latest-small' or ds == 'ml-1m':\n",
    "        archive_url = f'http://files.grouplens.org/datasets/movielens/{ds}.zip'\n",
    "        print(archive_url)\n",
    "        download_path = Path.home() / 'data' / 'movielens'\n",
    "        try_download(archive_url, download_path)\n",
    "        ratings, movies = read_data(download_path / ds)\n",
    "        movies.set_index('movieId', inplace=True)\n",
    "        ds_ = 'movielens'\n",
    "    \n",
    "    #binarize ratings\n",
    "    ratings['rating'] = binarize(ratings['rating'], ds_)\n",
    "    \n",
    "    #filter data (users and items)\n",
    "    for ff in range(3): # sufficient for our datasets\n",
    "        ratings = filter_dataset(ratings)\n",
    "    \n",
    "    #get test items for users\n",
    "    ratings, test_dict = create_test_dict(ratings)\n",
    "    \n",
    "    (n, m), (X, y_binary, t), (userId_to_index, movieId_to_index), \\\n",
    "    (index_to_movieId, index_to_userId) = create_dataset(ratings)\n",
    "    print(f'{n} users, {m} items')\n",
    "    \n",
    "    datasets = {'train': (X, y_binary)}\n",
    "    dataset_sizes = {'train': len(X)}\n",
    "    ratings_mat_full = create_user_movie_matrix(X, y_binary, n, m)   \n",
    "    \n",
    "    '''\n",
    "    Create lookup table for randomally select neg and pos samples for each user\n",
    "    Table will be: {useridx: [item1_idx, item2_idx, item3_idx, ...]} with the idx's being indexes of\n",
    "    neg/pos movies for that user'''\n",
    "    \n",
    "    print('Creating positive and negative examples for all users...')\n",
    "    pos_samples_idx_mat = np.argwhere(ratings_mat_full > 0).tolist()\n",
    "    neg_samples_idx_mat = np.argwhere(ratings_mat_full == 0).tolist()\n",
    "    \n",
    "    neg_samples_idx_dict = {}\n",
    "    for i in range(n):\n",
    "        neg_samples_idx_dict[i] = []\n",
    "    for neg_sample_id in neg_samples_idx_mat:\n",
    "        neg_samples_idx_dict[neg_sample_id[0]].append(neg_sample_id[1])\n",
    "    \n",
    "    pos_samples_idx_dict = {}\n",
    "    for i in range(n):\n",
    "        pos_samples_idx_dict[i] = []\n",
    "    for pos_sample_id in pos_samples_idx_mat:\n",
    "        pos_samples_idx_dict[pos_sample_id[0]].append(pos_sample_id[1])\n",
    "    \n",
    "    return (pos_samples_idx_dict, neg_samples_idx_dict, n, m,\n",
    "            datasets, dataset_sizes, test_dict,\n",
    "            userId_to_index, movieId_to_index, ratings, index_to_movieId, index_to_userId, movies)\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#################################################################################\n",
    "        \n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGIKEiPPv78I"
   },
   "source": [
    "# Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mhrUsm75vwqi"
   },
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_pos = 0.05, lamda_neg = 0.01, alpha = 0.5):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.lamda_pos = lamda_pos\n",
    "        self.lamda_neg = lamda_neg\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, model, users, items, prediction_input, labels, personas_scores_input):\n",
    "            \n",
    "        # The loss function without the entropy term over the personas vectors \n",
    "        if personas_scores_input is None:\n",
    "            CE_loss = self.loss_function(prediction_input, labels)\n",
    "            return CE_loss, CE_loss, torch.Tensor([0]), torch.Tensor([0])\n",
    "        \n",
    "        # The loss function for the AMP-CF model \n",
    "        CE_loss = self.loss_function(prediction_input, labels)\n",
    "        personas_scores = personas_scores_input.clone()\n",
    "        entropy_pos = torch.sum(Categorical(probs = personas_scores[:,:1,:]).entropy())\n",
    "        entropy_negs = torch.sum(torch.sum(Categorical(probs = personas_scores[:,1:,:]).entropy(), dim = -1) \\\n",
    "                                  / (personas_scores.shape[1] - 1))\n",
    "        total_loss = (self.alpha)*( CE_loss ) + (1-self.alpha)*( self.lamda_pos * entropy_pos - self.lamda_neg * entropy_negs )\n",
    "        return (total_loss, CE_loss, entropy_pos, entropy_negs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqzZ6iXMv8eh"
   },
   "source": [
    "# Collaborative Filtering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAtXcqbjvw2w"
   },
   "outputs": [],
   "source": [
    "class Collaborative_Filtering(torch.nn.Module):\n",
    "    \n",
    "    #### An object that contains all the models in the article (both initialization and forward) ####\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_factors, n_personas, num_samples, bs, pos_samples_idx_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_personas = n_personas\n",
    "        self.device = device\n",
    "        \n",
    "        # AMPCF\n",
    "        if n_personas > 1 and n_personas < 100:\n",
    "            self.emb_dimension = n_factors\n",
    "            self.user_factors = torch.nn.Parameter(torch.randn(n_users, n_personas, n_factors))\n",
    "            self.item_factors = torch.nn.Parameter(torch.randn(n_items, n_factors))\n",
    "\n",
    "            torch.nn.init.xavier_normal_(self.user_factors)\n",
    "            torch.nn.init.xavier_normal_(self.item_factors)\n",
    "        \n",
    "        # MF\n",
    "        if n_personas == 1:\n",
    "            self.emb_dimension = n_factors\n",
    "            self.user_factors = torch.nn.Parameter(torch.randn(n_users, n_personas, n_factors))\n",
    "            self.item_factors = torch.nn.Parameter(torch.randn(n_items, n_factors))\n",
    "            self.users_bias = torch.nn.Parameter(torch.randn(n_users))\n",
    "            self.items_bias = torch.nn.Parameter(torch.randn(n_items))\n",
    "\n",
    "            torch.nn.init.xavier_normal_(self.user_factors)\n",
    "            torch.nn.init.xavier_normal_(self.item_factors)\n",
    "        \n",
    "        # SpectralCF\n",
    "        if n_personas == 444:\n",
    "            self.n_factors = n_factors\n",
    "            self.user_factors = torch.nn.Parameter(torch.randn(n_users,n_factors))\n",
    "            self.item_factors = torch.nn.Parameter(torch.randn(n_items,n_factors))\n",
    "            torch.nn.init.xavier_normal_(self.user_factors)\n",
    "            torch.nn.init.xavier_normal_(self.item_factors)\n",
    "\n",
    "            interaction_matrix = torch.zeros([n_users,n_items])\n",
    "            for user_idx in range(n_users):\n",
    "                ind = pos_samples_idx_dict[user_idx]\n",
    "                interaction_matrix[user_idx][ind] = 1\n",
    "                \n",
    "            B1 = torch.cat((torch.zeros(n_users,n_users),interaction_matrix),1)\n",
    "            B2 = torch.cat((torch.transpose(interaction_matrix,0,1), torch.zeros(n_items,n_items)),1)\n",
    "            A = torch.cat((B1,B2),0)\n",
    "            D = torch.diag(torch.sum(A,1))\n",
    "            L = torch.eye(n_items+n_users)-torch.matmul(torch.inverse(D),A)\n",
    "            \n",
    "            G, U = torch.eig(L, eigenvectors=True) \n",
    "            G = torch.diag(G[:,0])\n",
    "            \n",
    "            Q = torch.matmul(U,torch.transpose(U,0,1))+torch.matmul(torch.matmul(U,G),torch.transpose(U,0,1))\n",
    "            self.Q = Q.requires_grad_(False).to(device)\n",
    "            \n",
    "            self.theta0 = torch.nn.Parameter(torch.randn(n_factors,n_factors))\n",
    "            self.theta1 = torch.nn.Parameter(torch.randn(n_factors,n_factors))\n",
    "            self.theta2 = torch.nn.Parameter(torch.randn(n_factors,n_factors))\n",
    "            torch.nn.init.xavier_normal_(self.theta0)\n",
    "            torch.nn.init.xavier_normal_(self.theta1)\n",
    "            torch.nn.init.xavier_normal_(self.theta2)\n",
    "            \n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        #JCA\n",
    "        if n_personas == 555:\n",
    "            \n",
    "            self.n_factors = n_factors\n",
    "            self.user_factors = torch.nn.Parameter(torch.randn(n_users, n_factors))\n",
    "            self.item_factors = torch.nn.Parameter(torch.randn(n_items, n_factors))\n",
    "\n",
    "            interaction_matrix = torch.zeros([n_users,n_items], requires_grad=False)\n",
    "            for user_idx in range(n_users):\n",
    "                ind = pos_samples_idx_dict[user_idx]\n",
    "                interaction_matrix[user_idx][ind] = 1\n",
    "                \n",
    "            self.interaction_matrix = interaction_matrix.to(device)\n",
    "            \n",
    "            torch.nn.init.xavier_normal_(self.user_factors)\n",
    "            torch.nn.init.xavier_normal_(self.item_factors)\n",
    "            \n",
    "            self.UW = torch.nn.Parameter(torch.randn(n_factors,n_items))\n",
    "            self.IW = torch.nn.Parameter(torch.randn(n_factors,n_users))\n",
    "            self.Ub1 = torch.nn.Parameter(torch.randn(1, n_factors))\n",
    "            self.Ub2 = torch.nn.Parameter(torch.randn(1,n_items))\n",
    "            self.Ib1 = torch.nn.Parameter(torch.randn(1, n_factors))\n",
    "            self.Ib2 = torch.nn.Parameter(torch.randn(1,n_users))\n",
    "            self.f = torch.nn.Parameter(torch.randn(n_items,1))\n",
    "            \n",
    "            torch.nn.init.xavier_normal_(self.UW)\n",
    "            torch.nn.init.xavier_normal_(self.IW)\n",
    "            torch.nn.init.xavier_normal_(self.Ub1)\n",
    "            torch.nn.init.xavier_normal_(self.Ub2)\n",
    "            torch.nn.init.xavier_normal_(self.Ib1)\n",
    "            torch.nn.init.xavier_normal_(self.Ib2)\n",
    "            torch.nn.init.xavier_normal_(self.f)\n",
    "            \n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        #CDAE\n",
    "        if n_personas == 666:\n",
    "            \n",
    "            self.n_factors = n_factors\n",
    "            self.user_factors = torch.nn.Parameter(torch.randn(n_users, n_factors))\n",
    "            self.item_factors = torch.nn.Parameter(torch.randn(n_items, n_factors))\n",
    "\n",
    "            interaction_matrix = torch.zeros([n_users,n_items], requires_grad=False)\n",
    "            for user_idx in range(n_users):\n",
    "                ind = pos_samples_idx_dict[user_idx]\n",
    "                interaction_matrix[user_idx][ind] = 1\n",
    "                \n",
    "            self.interaction_matrix = interaction_matrix.to(device)\n",
    "            \n",
    "            torch.nn.init.xavier_normal_(self.user_factors)\n",
    "            torch.nn.init.xavier_normal_(self.item_factors)\n",
    "            \n",
    "            self.b = torch.nn.Parameter(torch.randn(1,n_factors))\n",
    "            self.outlayer = torch.nn.Linear(in_features=int(n_factors), out_features=int(n_items))\n",
    "            torch.nn.init.xavier_normal_(self.b)\n",
    "            torch.nn.init.xavier_normal_(self.outlayer.weight)\n",
    "            \n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.dropout = torch.nn.Dropout(p=0.6)\n",
    "        \n",
    "        #ConvNCF    \n",
    "        if n_personas == 777:\n",
    "            self.user_count = n_users\n",
    "            self.item_count = n_items\n",
    "            self.num_samples = num_samples\n",
    "            self.embedding_size = 64\n",
    "            self.user_factors = torch.nn.Parameter(torch.randn(n_users, n_factors))\n",
    "            self.item_factors = torch.nn.Parameter(torch.randn(n_items, n_factors))\n",
    "\n",
    "            torch.nn.init.xavier_normal_(self.user_factors)\n",
    "            torch.nn.init.xavier_normal_(self.item_factors)\n",
    "\n",
    "            # cnn setting\n",
    "            self.channel_size = 32\n",
    "            self.kernel_size = 2\n",
    "            self.strides = 2\n",
    "            self.cnn = nn.Sequential(\n",
    "                # batch_size * 1 * 64 * 64\n",
    "                nn.Conv2d(1, self.channel_size, self.kernel_size, stride=self.strides),\n",
    "                nn.ReLU(),\n",
    "                # batch_size * 32 * 32 * 32\n",
    "                nn.Conv2d(self.channel_size, self.channel_size, self.kernel_size, stride=self.strides),\n",
    "                nn.ReLU(),\n",
    "                # batch_size * 32 * 16 * 16\n",
    "                nn.Conv2d(self.channel_size, self.channel_size, self.kernel_size, stride=self.strides),\n",
    "                nn.ReLU(),\n",
    "                # batch_size * 32 * 8 * 8\n",
    "                nn.Conv2d(self.channel_size, self.channel_size, self.kernel_size, stride=self.strides),\n",
    "                nn.ReLU(),\n",
    "                # batch_size * 32 * 4 * 4\n",
    "                nn.Conv2d(self.channel_size, self.channel_size, self.kernel_size, stride=self.strides),\n",
    "                nn.ReLU(),\n",
    "                # batch_size * 32 * 2 * 2\n",
    "                nn.Conv2d(self.channel_size, self.channel_size, self.kernel_size, stride=self.strides),\n",
    "                nn.ReLU(),\n",
    "                # batch_size * 32 * 1 * 1\n",
    "            )\n",
    "            \n",
    "            self.fc = nn.Linear(32, 1)\n",
    "            torch.nn.init.xavier_normal_(self.fc.weight)\n",
    "        \n",
    "        #NCF\n",
    "        if n_personas == 888:\n",
    "            \n",
    "            self.n_factors = n_factors\n",
    "            self.user_factors = torch.nn.Parameter(torch.randn(n_users, 5*n_factors)) #0:n_factors->GMF, n_factors:5*n_factors->MLP\n",
    "            self.item_factors = torch.nn.Parameter(torch.randn(n_items, 5*n_factors)) #0:n_factors->GMF, n_factors:5*n_factors->MLP\n",
    "            \n",
    "            torch.nn.init.xavier_normal_(self.user_factors)\n",
    "            torch.nn.init.xavier_normal_(self.item_factors)\n",
    "            \n",
    "            self.layer1 = torch.nn.Linear(in_features=int(8*n_factors), out_features=int(4*n_factors))\n",
    "            self.layer2 = torch.nn.Linear(in_features=int(4*n_factors), out_features=int(2*n_factors))\n",
    "            self.layer3 = torch.nn.Linear(in_features=int(2*n_factors), out_features=int(n_factors))\n",
    "            \n",
    "            self.affine_output = torch.nn.Linear(in_features=2*n_factors, out_features=1)\n",
    "            \n",
    "            torch.nn.init.xavier_normal_(self.layer1.weight)\n",
    "            torch.nn.init.xavier_normal_(self.layer2.weight)\n",
    "            torch.nn.init.xavier_normal_(self.layer3.weight)\n",
    "            torch.nn.init.xavier_normal_(self.affine_output.weight)\n",
    "            \n",
    "            self.relu =   torch.nn.ReLU()\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user, items, bs, n_personas, n_factors, test_flag = False):\n",
    "        \n",
    "        if n_personas == 1:\n",
    "            '''\n",
    "            Forward MF\n",
    "            '''\n",
    "            return self.forward_MF(user, items, test_flag)\n",
    "        \n",
    "        if n_personas > 1 and n_personas < 100:\n",
    "            '''\n",
    "            Forward AMPCF\n",
    "            '''\n",
    "            return self.forward_AMPCF(user, items, test_flag)\n",
    "        \n",
    "        if n_personas == 444:\n",
    "            '''\n",
    "            Forward SpectralCF\n",
    "            '''\n",
    "            return self.forward_SpectralCF(user, items, test_flag)\n",
    "        \n",
    "        if n_personas == 555:\n",
    "            '''\n",
    "            Forward JCA\n",
    "            '''\n",
    "            return self.forward_JCA(user, items, test_flag)\n",
    "        \n",
    "        if n_personas == 666:\n",
    "            '''\n",
    "            Forward CDAE\n",
    "            '''\n",
    "            return self.forward_CDAE(user, items, test_flag)\n",
    "        \n",
    "        if n_personas == 777:\n",
    "            '''\n",
    "            Forward ConvNCF\n",
    "            '''\n",
    "            return self.forward_ConvNCF(user, items, test_flag)\n",
    "        \n",
    "        if n_personas == 888:\n",
    "            '''\n",
    "            Forward NeuMF\n",
    "            '''\n",
    "            return self.forward_NeuMF(user, items, test_flag)\n",
    "        \n",
    "    def forward_AMPCF(self, user, items, test_flag):\n",
    "        \n",
    "        if test_flag == False:\n",
    "            n_personas = self.n_personas                \n",
    "            u = self.user_factors[user].squeeze()  # 256,4,128\n",
    "            v = self.item_factors[items] # 256,5,128\n",
    "            r = torch.einsum('bsf,bpf->bsp', [v, u])\n",
    "            attentive_scores = F.softmax(r,dim=-1)\n",
    "            attentive_user = torch.einsum('bpf,bsp->bsf',[u,attentive_scores])\n",
    "            pred = torch.einsum('bsf,bsf->bs', [attentive_user,v])\n",
    "            return(pred,attentive_scores)\n",
    "            \n",
    "        if test_flag == True:\n",
    "            u = self.user_factors[user].squeeze()\n",
    "            v = self.item_factors[items].squeeze()\n",
    "            r = torch.einsum('pf,bf->bp', [u, v])\n",
    "            attentive_scores = F.softmax(r,dim=-1)\n",
    "            attentive_user = torch.einsum('pf,bp->bf',[u,attentive_scores])\n",
    "            pred = torch.einsum('bf,bf->b', [attentive_user,v])\n",
    "            return(pred.squeeze(),attentive_scores,r)\n",
    "    \n",
    "    def forward_SpectralCF(self, user, items, test_flag = False):\n",
    "        \n",
    "        X0 = torch.cat((self.user_factors,self.item_factors),0)\n",
    "        X1 = torch.sigmoid(torch.matmul(torch.matmul(self.Q,X0),self.theta0))\n",
    "        X2 = torch.sigmoid(torch.matmul(torch.matmul(self.Q,X1),self.theta1))\n",
    "        X3 = torch.sigmoid(torch.matmul(torch.matmul(self.Q,X2),self.theta2))\n",
    "        \n",
    "        V = torch.cat((X0,X1,X2,X3),1)\n",
    "        Vu = V[0:len(self.user_factors),:]\n",
    "        Vi = V[len(self.user_factors):,:]\n",
    "        \n",
    "        if test_flag == False:\n",
    "            score = torch.einsum('bsf,bpf->bs', [Vi[items,:], Vu[user,:]])\n",
    "        \n",
    "        if test_flag == True:\n",
    "            score = torch.matmul(Vi[items.squeeze(),:],Vu[user.squeeze(),:].squeeze())\n",
    "            \n",
    "        if test_flag:\n",
    "            return score.squeeze(),None,None\n",
    "        return score.squeeze(),None \n",
    "    \n",
    "    def forward_JCA(self,user,items,test_flag = False):\n",
    "        \n",
    "        interaction_batch1 = self.interaction_matrix[user.squeeze()]\n",
    "        interaction_batch2 = torch.transpose(self.interaction_matrix,0,1)[items.squeeze()]\n",
    "        \n",
    "        if test_flag == False:\n",
    "            \n",
    "            Z1 = self.sigmoid(torch.einsum('bi,ik->bk',[interaction_batch1,self.item_factors]) + self.Ub1)\n",
    "            score1 = self.sigmoid(torch.matmul(Z1,self.UW)+self.Ub2)\n",
    "            \n",
    "            Z2 = self.sigmoid(torch.mul(torch.einsum('bau,uk->bak',[interaction_batch2,self.user_factors]) + self.Ib1, self.f[items.squeeze()]))\n",
    "            score2 = self.sigmoid(torch.matmul(Z2,self.IW)+self.Ib2)\n",
    "            \n",
    "            score = 0.5*(torch.gather(score1, 1, items) + torch.gather(score2, 2, user.expand(score2.size(0),score2.size(1)).unsqueeze(2)).squeeze())\n",
    "            \n",
    "        if test_flag == True:\n",
    "            interaction_batch1 = interaction_batch1.unsqueeze(0)\n",
    "            interaction_batch2 = interaction_batch2.unsqueeze(0)\n",
    "            \n",
    "            Z1 = self.sigmoid(torch.einsum('bi,ik->bk',[interaction_batch1,self.item_factors]) + self.Ub1)\n",
    "            score1 = self.sigmoid(torch.matmul(Z1,self.UW)+self.Ub2).squeeze()\n",
    "            \n",
    "            Z2 = self.sigmoid(torch.mul(torch.einsum('bau,uk->bak',[interaction_batch2,self.user_factors]) + self.Ib1,self.f[items.squeeze()]))\n",
    "            score2 = self.sigmoid(torch.matmul(Z2,self.IW)+self.Ib2).squeeze()\n",
    "            \n",
    "            score = 0.5*(score1[items.squeeze()] + score2[:,user.squeeze()].squeeze())\n",
    "            \n",
    "        if test_flag:\n",
    "            return score.squeeze(),None,None\n",
    "        return score.squeeze(),None \n",
    "\n",
    "    def forward_CDAE(self,user,items,test_flag = False):\n",
    "        \n",
    "        interaction_batch = self.interaction_matrix[user.squeeze()].clone()\n",
    "        \n",
    "        if test_flag == False:\n",
    "            interaction_batch = self.dropout(interaction_batch) # drop-out with prob 0.6\n",
    "            Z = self.sigmoid(torch.matmul(interaction_batch, self.item_factors) + self.user_factors[user.squeeze()] + self.b)\n",
    "            Y = self.sigmoid(self.outlayer(Z))\n",
    "            score = torch.gather(Y, 1, items)\n",
    "            \n",
    "        if test_flag == True:\n",
    "            interaction_batch = interaction_batch.unsqueeze(0)\n",
    "            Z = self.sigmoid(torch.matmul(interaction_batch,self.item_factors) + self.user_factors[user.squeeze()].unsqueeze(0) + self.b)\n",
    "            Y = self.sigmoid(self.outlayer(Z)).squeeze()\n",
    "            score = Y[items]\n",
    "        \n",
    "        if test_flag:\n",
    "            return score.squeeze(),None,None\n",
    "        return score.squeeze(),None \n",
    "    \n",
    "    def forward_NeuMF(self, user, items, test_flag = False):\n",
    "        \n",
    "        user_embedding = self.user_factors[user]\n",
    "        item_embedding = self.item_factors[items]\n",
    "        \n",
    "        if test_flag:\n",
    "            user_embedding = user_embedding.unsqueeze(0)\n",
    "            item_embedding = item_embedding.unsqueeze(0)\n",
    "            \n",
    "        user_embedding_GMF = user_embedding[:,:,0:self.n_factors]\n",
    "        item_embedding_GMF = item_embedding[:,:,0:self.n_factors]\n",
    "        user_embedding_MLP = user_embedding[:,:,self.n_factors:5*self.n_factors]\n",
    "        item_embedding_MLP = item_embedding[:,:,self.n_factors:5*self.n_factors]\n",
    "            \n",
    "        element_product = torch.mul(user_embedding_GMF, item_embedding_GMF)\n",
    "        \n",
    "        MLP_vec = torch.cat((user_embedding_MLP.repeat(1, item_embedding_MLP.shape[1], 1), item_embedding_MLP), dim=2)\n",
    "        aft1 = self.relu(self.layer1(MLP_vec))\n",
    "        aft2 = self.relu(self.layer2(aft1))\n",
    "        aft3 = self.relu(self.layer3(aft2))\n",
    "        \n",
    "        concat_vec = torch.cat((element_product,aft3), dim=2)  \n",
    "        aft4 = self.sigmoid(self.affine_output(concat_vec))\n",
    "        \n",
    "        if test_flag:\n",
    "            return aft4.squeeze(),None,None\n",
    "        \n",
    "        return aft4.squeeze(),None\n",
    "\n",
    "    def forward_ConvNCF(self, user_ids, item_ids, test_flag = False):\n",
    "\n",
    "        user_embeddings = self.user_factors[user_ids]\n",
    "        item_embeddings = self.item_factors[item_ids]\n",
    "        \n",
    "        if test_flag:\n",
    "            user_embeddings = user_embeddings.unsqueeze(0)\n",
    "            item_embeddings = item_embeddings.unsqueeze(0)\n",
    "        \n",
    "        num_samples = item_embeddings.shape[1]  \n",
    "        interaction_map = torch.einsum('buf,bsg->bsfg',[user_embeddings,item_embeddings])\n",
    "        interaction_map = interaction_map.view((-1, 1, self.embedding_size, self.embedding_size))    \n",
    "        feature_map = self.cnn(interaction_map)\n",
    "        feature_vec = feature_map.view((-1, 32))\n",
    "        prediction = self.fc(feature_vec)    \n",
    "        prediction = prediction.view((-1,num_samples))\n",
    "        \n",
    "        if test_flag:\n",
    "            return prediction.squeeze(),None,None\n",
    "        return prediction, None        \n",
    "        \n",
    "        \n",
    "    def forward_MF(self, user, items, test_flag = False):\n",
    "        '''\n",
    "        Baseline model with single user vector (ie (1,128))\n",
    "        '''\n",
    "        u = self.user_factors[user].squeeze()  \n",
    "        v = self.item_factors[items] \n",
    "        if test_flag:\n",
    "            u = u.squeeze()\n",
    "            v = v.squeeze()\n",
    "            bias = self.users_bias[user] + self.items_bias[items]\n",
    "            return torch.einsum('sf,f->s',[v, u]) + bias,None,None\n",
    "        else:\n",
    "            bias = self.users_bias[user] + self.items_bias[items]\n",
    "            return torch.einsum('bsf,bf->bs',[v, u]) + bias,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6B8c_Nev9Ac"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vhuz_kIVvxFg"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    '''This object contains the trained model, the variables and the functions as explained in the comments.\n",
    "       Most variables are initialized during the run or set at the end.\n",
    "       The user has the option to change the following fields in the object:\n",
    "       num_neg_samples, num_random_samples, k.'''\n",
    "    \n",
    "    def __init__(self, n_factors, n_personas, pos_samples_idx_dict, neg_samples_idx_dict, n, m,\n",
    "                 datasets, dataset_sizes, test_dict, userId_to_index, \n",
    "                 movieId_to_index, ratings, index_to_movieId, \n",
    "                 index_to_userId, num_epochs, bs, lr, alpha, lamda_pos, lamda_neg, patience):\n",
    "        \n",
    "        self.device = device\n",
    "        self.n = n  # num of users\n",
    "        self.m = m  # num of items\n",
    "        self.n_factors = n_factors  # number of latent dimensions\n",
    "        self.n_personas = n_personas  # number of personas per user or model indicator\n",
    "        self.bs = bs  # batch size\n",
    "        self.patience = patience # number of epochs to stop the training (HR/NDCG saturation)\n",
    "        self.n_epochs = num_epochs # number of epochs\n",
    "        self.lr = lr  # learning rate\n",
    "        self.wd = 0 # weight decay in Adam optimize\n",
    "        self.alpha = alpha # a hyperparameter in the loss function\n",
    "        self.lamda_pos = lamda_pos # a hyperparameter in the loss function\n",
    "        self.lamda_neg = lamda_neg # a hyperparameter in the loss function\n",
    "        self.num_neg_samples = 4 # number of negative items per positive item \n",
    "        self.num_samples = self.num_neg_samples + 1 # negative samples + positive sample\n",
    "        self.num_random_samples = 99 # number of random items in test (the 100th item is the test item)\n",
    "        self.k = 10  # HR@K \n",
    "        self.losses = [] # loss array\n",
    "        self.target = torch.LongTensor([0] * self.bs).to(self.device) # loss function labels\n",
    "        self.loss_func = CustomLoss(alpha = alpha, lamda_pos = lamda_pos, lamda_neg = lamda_neg) # the loss function\n",
    "        self.hr_arr = [] # saves the HR results\n",
    "        self.best_hr = 0 # the best HR in hr_arr\n",
    "        self.mean_hr = 0 # mean HR at the end of the run (with respect to patience)\n",
    "        self.ndcg_arr = [] # saves the NDCG results\n",
    "        self.best_ndcg = 0 # the best NDCG in ndcg_arr\n",
    "        self.mean_ndcg = 0 # mean NDCG at the end of the run (with respect to patience)\n",
    "        self.userId_to_index = userId_to_index # user ID to index\n",
    "        self.index_to_userId = index_to_userId # user index to ID\n",
    "        self.movieId_to_index = movieId_to_index # item ID to inde\n",
    "        self.index_to_movieId = index_to_movieId # item index to ID\n",
    "        self.datasets = datasets # the dataset of the model after data processing\n",
    "        self.dataset_sizes = dataset_sizes # number of train samples\n",
    "        self.top_movies_idxs_everyone = None # list of top 30 movies (indices) for each user \n",
    "        self.batches_dict = {} # batches dictionary\n",
    "        self.pos_samples_idx_dict = pos_samples_idx_dict # positive samples for each user\n",
    "        self.neg_samples_idx_dict = neg_samples_idx_dict # negative samples for each user\n",
    "        self.test_dict = test_dict # dictionary of users and their test items\n",
    "        self.test_hashtable = self.get_test_hashtable() # the test items for each user\n",
    "        self.model = Collaborative_Filtering(n, m, n_factors, n_personas, self.num_samples, self.bs, pos_samples_idx_dict).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.wd,\n",
    "                                          amsgrad=False)  # the optimizer\n",
    "\n",
    "    def get_test_hashtable(self):\n",
    "        test_hashtable = {}\n",
    "        for userId in self.test_dict:\n",
    "            user_idx = self.userId_to_index[userId]\n",
    "            test_hashtable[user_idx] = set()\n",
    "            for movieId in self.test_dict[userId]:\n",
    "                if movieId not in self.movieId_to_index:\n",
    "                    continue\n",
    "                movie_idx = self.movieId_to_index[movieId]\n",
    "                test_hashtable[user_idx].add(movie_idx)\n",
    "        return test_hashtable\n",
    "    \n",
    "    def hit(self, gt_item, pred_items): #hit ratio\n",
    "        pred_items = set(pred_items)\n",
    "        if gt_item in pred_items:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def ndcg(self, gt_item, pred_items):\n",
    "        pred_items_set = set(pred_items)\n",
    "        if gt_item in pred_items_set:\n",
    "            index = pred_items.index(gt_item)\n",
    "            return np.reciprocal(np.log2(index+2))\n",
    "        return 0\n",
    "                \n",
    "    def HR_and_NDCG_at_K(self, K): #calculating the HR and NDCG\n",
    "        HR = []\n",
    "        NDCG = []\n",
    "        for user_idx in self.test_hashtable:\n",
    "            test_items = list(self.test_hashtable[user_idx])\n",
    "            num_test_items = len(test_items)\n",
    "            if num_test_items == 0:\n",
    "                continue\n",
    "            user_hr = 0\n",
    "            user_ndcg = 0\n",
    "            random.seed(user_idx)\n",
    "            for item in test_items:\n",
    "                random_items = random.sample(self.neg_samples_idx_dict[user_idx], k = 99)\n",
    "                items = [item] + random_items\n",
    "                user_scores, _, _ = self.model(torch.LongTensor([user_idx]).to(self.device),\n",
    "                                                     torch.LongTensor(items).to(self.device), 1,\n",
    "                                                     self.n_personas, self.n_factors, test_flag = True)\n",
    "                \n",
    "                _, indices = torch.sort(user_scores,descending=True)\n",
    "                recommend_items = torch.take(torch.LongTensor(items), indices.cpu()).cpu().numpy().tolist()\n",
    "                \n",
    "                user_hr += self.hit(item, recommend_items[0:K])\n",
    "                user_ndcg += self.ndcg(item, recommend_items[0:K])\n",
    "                \n",
    "            HR.append(user_hr / num_test_items)\n",
    "            NDCG.append(user_ndcg / num_test_items)\n",
    "        return np.mean(HR), np.mean(NDCG)\n",
    "\n",
    "    def get_top_k_movies_idxs_low_ram(self): # Gives the top 30 movies to each user\n",
    "        k = 30\n",
    "        user_idxs = [i for i in range(self.n)]\n",
    "        num_users = self.n\n",
    "        real_top_k = np.zeros(shape = (num_users,k), dtype = int)\n",
    "        real_top_k_scores = np.zeros(shape = (num_users,k), dtype = float)\n",
    "        all_movies_idxs = range(self.m)  \n",
    "        for user_idx in user_idxs:\n",
    "            scores , _, _ = self.model(torch.LongTensor([user_idx]).to(self.device),\n",
    "                                                torch.LongTensor(all_movies_idxs).to(self.device), 1,\n",
    "                                                self.n_personas, self.n_factors, test_flag = True)\n",
    "            top_k_movies_idxs = (-scores.detach().cpu().numpy()).argsort()\n",
    "            ki = 0\n",
    "            for movie_idx in top_k_movies_idxs:\n",
    "                if ki == k:\n",
    "                    break\n",
    "                if int(movie_idx) in set(self.pos_samples_idx_dict[user_idx]):                  \n",
    "                    continue\n",
    "                else:\n",
    "                    real_top_k[user_idx][ki] = int(movie_idx)\n",
    "                    real_top_k_scores[user_idx][ki] = scores[movie_idx]\n",
    "                    ki += 1\n",
    "        self.top_movies_idxs_everyone = real_top_k\n",
    "\n",
    "    def train_model(self):         \n",
    "        \n",
    "        num_epochs_no_improvement = 0\n",
    "        patience = self.patience\n",
    "        self.model.train()\n",
    "        self.batches_dict = prep_batches(self.datasets, self.bs)\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            if num_epochs_no_improvement+1 > patience:   \n",
    "                self.mean_hr = (sum(self.hr_arr[(-1*patience-1):]) / (patience+1))\n",
    "                self.mean_ndcg = (sum(self.ndcg_arr[(-1*patience-1):]) / (patience+1))\n",
    "                self.best_hr = max(self.hr_arr)\n",
    "                self.best_ndcg = max(self.ndcg_arr)\n",
    "                break\n",
    "            \n",
    "            print(\"Epoch: {}\".format(epoch + 1), end='\\n', flush=True)\n",
    "            stats = {'epoch': epoch + 1, 'total': self.n_epochs}\n",
    "\n",
    "            phase = 'train'\n",
    "            training = True\n",
    "            running_loss_ce = 0.0\n",
    "            running_loss_ent_pos = 0.0\n",
    "            running_loss_ent_neg = 0.0\n",
    "            running_loss = 0.0\n",
    "            n_batches = 0\n",
    "            i = 0\n",
    "\n",
    "            for x_batch in self.batches_dict['train']['x_batches']:\n",
    "                self.optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(training):  # compute gradients only during 'train' phase\n",
    "                    batch_user_idxs = x_batch.T[0]  # bs = 5 --> [4, 312, 788, 14, 15]\n",
    "                    batch_item_idxs = []\n",
    "                    for user_idx in batch_user_idxs:\n",
    "                        neg_movies_idxs = random.sample(self.neg_samples_idx_dict[user_idx],\n",
    "                                                        k=self.num_neg_samples)  # k =4 -> [18, 1898, 8900, 31]\n",
    "                        pos_movie_idx = random.sample(self.pos_samples_idx_dict[user_idx], k=1)  # [8762]\n",
    "                        item_idxs = pos_movie_idx + neg_movies_idxs  # [8762, 18, 1898, 8900, 31]\n",
    "                        batch_item_idxs.append(item_idxs)\n",
    "\n",
    "                    users = torch.LongTensor([batch_user_idxs]).to(self.device)\n",
    "                    items = torch.LongTensor(batch_item_idxs).to(self.device)\n",
    "                    \n",
    "                    predictions, personas_scores = self.model(users.T, items, self.bs, self.n_personas, self.n_factors)\n",
    "                    loss, CE_loss, entropy_pos, entropy_negs  = self.loss_func(self.model,self.model.user_factors[users].reshape(-1),self.model.item_factors[items].reshape(-1),predictions, self.target, personas_scores) # Shape = [], ie a scalar\n",
    "                    \n",
    "                    # ce_loss, entropy_loss\n",
    "                    # don't update weights and rates when in 'val' phase\n",
    "                    if training:\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "                        del users\n",
    "                        del items\n",
    "                        del predictions\n",
    "                        del personas_scores\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_loss_ce += CE_loss.item()\n",
    "                running_loss_ent_pos += entropy_pos.item()\n",
    "                running_loss_ent_neg += entropy_negs.item()\n",
    "                \n",
    "                self.losses.append(loss.item() / self.bs)\n",
    "                \n",
    "                epoch_loss = running_loss / self.dataset_sizes[phase]\n",
    "                epoch_loss_ce = running_loss_ce / self.dataset_sizes[phase]\n",
    "                epoch_loss_pos = running_loss_ent_pos / self.dataset_sizes[phase]\n",
    "                epoch_loss_neg = running_loss_ent_neg / self.dataset_sizes[phase]\n",
    "                \n",
    "                stats[phase] = epoch_loss            \n",
    "            print(\"CE Loss: {:.4f} Entr Pos Loss: {:.4f} Entr Neg Loss: {:.4f}\\n\".format(\n",
    "                epoch_loss_ce, epoch_loss_pos, epoch_loss_neg), end='', flush=True)\n",
    "\n",
    "            '''HR and NDCG @ K Calc'''\n",
    "            zero_hr, zero_ndcg = self.HR_and_NDCG_at_K(self.k)\n",
    "            self.hr_arr.append(zero_hr)\n",
    "            self.ndcg_arr.append(zero_ndcg)\n",
    "            improved_hr = False\n",
    "            improved_ndcg = False\n",
    "            num_epochs_no_improvement += 1 \n",
    "\n",
    "            print('HR: {:.4f}'.format(zero_hr), end='  ', flush=True)\n",
    "            print('NDCG: {:.4f}'.format(zero_ndcg), end='  ', flush=True)\n",
    "\n",
    "            if self.best_hr < zero_hr:\n",
    "                self.best_hr = zero_hr\n",
    "                num_epochs_no_improvement = 0\n",
    "                improved_hr = True\n",
    "                \n",
    "            if self.best_ndcg < zero_ndcg:\n",
    "                self.best_ndcg = zero_ndcg\n",
    "                num_epochs_no_improvement = 0\n",
    "                improved_ndcg = True\n",
    "                    \n",
    "            if improved_hr:\n",
    "                print('HR improved', end=' ', flush=True)\n",
    "            if improved_ndcg:\n",
    "                print('NDCG improved', end=' ', flush=True)\n",
    "            print('\\n')\n",
    "\n",
    "##########################################################################################\n",
    "########################### After Training the models ####################################\n",
    "##########################################################################################\n",
    "    \n",
    "    '''One can get the recommendation list of a user using: model.get_recommended_movies_names(user_idx),\n",
    "       the list of positive items of a user using: model.get_training_movies_names(user_idx),\n",
    "       and the test item of each user using: model.get_test_movies_names(user_idx).'''\n",
    "    \n",
    "    def get_recommended_movies_names(self, user_idx):\n",
    "        movieIds_arr = []\n",
    "        movie_names_arr = []\n",
    "        userId = self.index_to_userId[user_idx]\n",
    "        print(f\"userId = {userId}, user_idx = {user_idx} in embedding matrix\")\n",
    "        movie_genre_arr = []\n",
    "        for index,idx in enumerate(self.top_movies_idxs_everyone[user_idx][0:30]): \n",
    "            movieId = self.index_to_movieId[idx.item()]\n",
    "            movieIds_arr.append(movieId)\n",
    "            movie_name = movies.loc[movieId,:][0]\n",
    "            movie_genre = movies.loc[movieId,:][1]\n",
    "            movie_genre_arr.append([movie_name, movie_genre, idx])\n",
    "        print(tabulate(movie_genre_arr, headers=['Movie Name', 'Genre', 'movieIdx']))\n",
    "\n",
    "    def get_training_movies_names(self, user_idx):\n",
    "        movieIds_arr = []\n",
    "        movie_names_arr = []\n",
    "        userId = self.index_to_userId[user_idx]\n",
    "        print(f\"userId = {userId}, user_idx = {user_idx} in embedding matrix\")\n",
    "        print('#### Training Movie Names ####')\n",
    "        movie_genre_arr = []\n",
    "        for idx in set(self.pos_samples_idx_dict[user_idx]):\n",
    "            if idx not in self.test_hashtable[user_idx]:\n",
    "                movieId = self.index_to_movieId[idx]\n",
    "                movieIds_arr.append(movieId)\n",
    "                movie_name = movies.loc[movieId,:][0]\n",
    "                movie_genre = movies.loc[movieId,:][1]\n",
    "                movie_genre_arr.append([movie_name, movie_genre, idx])\n",
    "        print(tabulate(movie_genre_arr, headers=['Movie Name', 'Genre', 'movieIdx']))\n",
    "\n",
    "    def get_test_movies_names(self, user_idx):\n",
    "        movieIds_arr = []\n",
    "        movie_names_arr = []\n",
    "        userId = self.index_to_userId[user_idx]\n",
    "        print(f\"userId = {userId}, user_idx = {user_idx} in embedding matrix\")\n",
    "        movie_genre_arr = []\n",
    "        for idx in self.test_hashtable[user_idx]:\n",
    "            movieId = self.index_to_movieId[idx]\n",
    "            movieIds_arr.append(movieId)\n",
    "            movie_name = movies.loc[movieId,:][0]\n",
    "            movie_genre = movies.loc[movieId,:][1]\n",
    "            movie_genre_arr.append([movie_name, movie_genre, idx])\n",
    "        print(tabulate(movie_genre_arr, headers=['Movie Name', 'Genre', 'movieIdx']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VGFO2sswoND"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OPKRkSK4wobk",
    "outputId": "9c77295d-23f2-446e-ea05-d0ac84ab33d5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training on Yahoo Music\n",
      "Filtering Data\n",
      "Filtering Data\n",
      "Filtering Data\n",
      "Amount of training samples:  398141\n",
      "14999 users, 5394 items\n",
      "Creating positive and negative examples for all users...\n",
      "\n",
      " -------Data Prep Finished, Starting Training--------\n",
      "\n",
      "model type, #latent dimensions, alpha, positive lambda, negative lambda: 1 8 1 0 0\n",
      "\n",
      "# of training batches:  1555\n",
      "Epoch: 1\n",
      "CE Loss: 0.0050 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.7201  NDCG: 0.4527  HR improved NDCG improved \n",
      "\n",
      "Epoch: 2\n",
      "CE Loss: 0.0022 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.8348  NDCG: 0.5512  HR improved NDCG improved \n",
      "\n",
      "Epoch: 3\n",
      "CE Loss: 0.0019 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.8507  NDCG: 0.5671  HR improved NDCG improved \n",
      "\n",
      "Epoch: 4\n",
      "CE Loss: 0.0018 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.8561  NDCG: 0.5750  HR improved NDCG improved \n",
      "\n",
      "Epoch: 5\n",
      "CE Loss: 0.0017 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.8634  NDCG: 0.5873  HR improved NDCG improved \n",
      "\n",
      "Epoch: 6\n",
      "CE Loss: 0.0015 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.8699  NDCG: 0.6012  HR improved NDCG improved \n",
      "\n",
      "Epoch: 7\n",
      "CE Loss: 0.0014 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.8768  NDCG: 0.6141  HR improved NDCG improved \n",
      "\n",
      "Epoch: 8\n",
      "CE Loss: 0.0013 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.8845  NDCG: 0.6268  HR improved NDCG improved \n",
      "\n",
      "Epoch: 9\n",
      "CE Loss: 0.0012 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.8899  NDCG: 0.6355  HR improved NDCG improved \n",
      "\n",
      "Epoch: 10\n",
      "CE Loss: 0.0011 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.8954  NDCG: 0.6453  HR improved NDCG improved \n",
      "\n",
      "Epoch: 11\n",
      "CE Loss: 0.0010 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9002  NDCG: 0.6540  HR improved NDCG improved \n",
      "\n",
      "Epoch: 12\n",
      "CE Loss: 0.0009 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9039  NDCG: 0.6625  HR improved NDCG improved \n",
      "\n",
      "Epoch: 13\n",
      "CE Loss: 0.0009 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9094  NDCG: 0.6705  HR improved NDCG improved \n",
      "\n",
      "Epoch: 14\n",
      "CE Loss: 0.0008 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9117  NDCG: 0.6750  HR improved NDCG improved \n",
      "\n",
      "Epoch: 15\n",
      "CE Loss: 0.0008 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9147  NDCG: 0.6798  HR improved NDCG improved \n",
      "\n",
      "Epoch: 16\n",
      "CE Loss: 0.0007 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9165  NDCG: 0.6828  HR improved NDCG improved \n",
      "\n",
      "Epoch: 17\n",
      "CE Loss: 0.0007 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9178  NDCG: 0.6859  HR improved NDCG improved \n",
      "\n",
      "Epoch: 18\n",
      "CE Loss: 0.0006 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9195  NDCG: 0.6884  HR improved NDCG improved \n",
      "\n",
      "Epoch: 19\n",
      "CE Loss: 0.0006 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9197  NDCG: 0.6896  HR improved NDCG improved \n",
      "\n",
      "Epoch: 20\n",
      "CE Loss: 0.0006 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9197  NDCG: 0.6901  NDCG improved \n",
      "\n",
      "Epoch: 21\n",
      "CE Loss: 0.0005 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9195  NDCG: 0.6918  NDCG improved \n",
      "\n",
      "Epoch: 22\n",
      "CE Loss: 0.0005 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9199  NDCG: 0.6926  HR improved NDCG improved \n",
      "\n",
      "Epoch: 23\n",
      "CE Loss: 0.0005 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9204  NDCG: 0.6933  HR improved NDCG improved \n",
      "\n",
      "Epoch: 24\n",
      "CE Loss: 0.0005 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9201  NDCG: 0.6932  \n",
      "\n",
      "Epoch: 25\n",
      "CE Loss: 0.0004 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9200  NDCG: 0.6936  NDCG improved \n",
      "\n",
      "Epoch: 26\n",
      "CE Loss: 0.0004 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9193  NDCG: 0.6931  \n",
      "\n",
      "Epoch: 27\n",
      "CE Loss: 0.0004 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9190  NDCG: 0.6929  \n",
      "\n",
      "Epoch: 28\n",
      "CE Loss: 0.0004 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9189  NDCG: 0.6930  \n",
      "\n",
      "Epoch: 29\n",
      "CE Loss: 0.0004 Entr Pos Loss: 0.0000 Entr Neg Loss: 0.0000\n",
      "HR: 0.9188  NDCG: 0.6925  \n",
      "\n",
      "\n",
      " ######################################################## \n",
      "\n",
      "model type, #latent dimensions, alpha, positive lambda, negative lambda: 3 64 0.5 0.005 0.005\n",
      "\n",
      "# of training batches:  1555\n",
      "Epoch: 1\n",
      "CE Loss: 0.0033 Entr Pos Loss: 0.9763 Entr Neg Loss: 1.0672\n",
      "HR: 0.8740  NDCG: 0.6228  HR improved NDCG improved \n",
      "\n",
      "Epoch: 2\n",
      "CE Loss: 0.0016 Entr Pos Loss: 0.3644 Entr Neg Loss: 0.9885\n",
      "HR: 0.8965  NDCG: 0.6621  HR improved NDCG improved \n",
      "\n",
      "Epoch: 3\n",
      "CE Loss: 0.0013 Entr Pos Loss: 0.1747 Entr Neg Loss: 0.9831\n",
      "HR: 0.9089  NDCG: 0.6802  HR improved NDCG improved \n",
      "\n",
      "Epoch: 4\n",
      "CE Loss: 0.0011 Entr Pos Loss: 0.1246 Entr Neg Loss: 0.9747\n",
      "HR: 0.9177  NDCG: 0.6941  HR improved NDCG improved \n",
      "\n",
      "Epoch: 5\n",
      "CE Loss: 0.0009 Entr Pos Loss: 0.0966 Entr Neg Loss: 0.9783\n",
      "HR: 0.9251  NDCG: 0.7050  HR improved NDCG improved \n",
      "\n",
      "Epoch: 6\n",
      "CE Loss: 0.0007 Entr Pos Loss: 0.0754 Entr Neg Loss: 0.9858\n",
      "HR: 0.9289  NDCG: 0.7141  HR improved NDCG improved \n",
      "\n",
      "Epoch: 7\n",
      "CE Loss: 0.0005 Entr Pos Loss: 0.0592 Entr Neg Loss: 0.9937\n",
      "HR: 0.9339  NDCG: 0.7219  HR improved NDCG improved \n",
      "\n",
      "Epoch: 8\n",
      "CE Loss: 0.0004 Entr Pos Loss: 0.0475 Entr Neg Loss: 1.0010\n",
      "HR: 0.9355  NDCG: 0.7253  HR improved NDCG improved \n",
      "\n",
      "Epoch: 9\n",
      "CE Loss: 0.0003 Entr Pos Loss: 0.0388 Entr Neg Loss: 1.0074\n",
      "HR: 0.9363  NDCG: 0.7273  HR improved NDCG improved \n",
      "\n",
      "Epoch: 10\n",
      "CE Loss: 0.0002 Entr Pos Loss: 0.0323 Entr Neg Loss: 1.0131\n",
      "HR: 0.9375  NDCG: 0.7280  HR improved NDCG improved \n",
      "\n",
      "Epoch: 11\n",
      "CE Loss: 0.0002 Entr Pos Loss: 0.0273 Entr Neg Loss: 1.0181\n",
      "HR: 0.9367  NDCG: 0.7280  NDCG improved \n",
      "\n",
      "Epoch: 12\n",
      "CE Loss: 0.0001 Entr Pos Loss: 0.0235 Entr Neg Loss: 1.0225\n",
      "HR: 0.9359  NDCG: 0.7277  \n",
      "\n",
      "Epoch: 13\n",
      "CE Loss: 0.0001 Entr Pos Loss: 0.0206 Entr Neg Loss: 1.0264\n",
      "HR: 0.9352  NDCG: 0.7275  \n",
      "\n",
      "Epoch: 14\n",
      "CE Loss: 0.0001 Entr Pos Loss: 0.0183 Entr Neg Loss: 1.0299\n",
      "HR: 0.9347  NDCG: 0.7268  \n",
      "\n",
      "Epoch: 15\n",
      "CE Loss: 0.0001 Entr Pos Loss: 0.0166 Entr Neg Loss: 1.0331\n",
      "HR: 0.9334  NDCG: 0.7259  \n",
      "\n",
      "\n",
      " ######################################################## \n",
      "\n",
      "Model: (1, 8, 1, 0, 0)\n",
      "Best HR: 0.9204 ||  Best NDCG: 0.6936\n",
      "\n",
      "\n",
      "Model: (3, 64, 0.5, 0.005, 0.005)\n",
      "Best HR: 0.9375 ||  Best NDCG: 0.7280\n",
      "\n",
      "\n",
      "############## Finished Training Dataset Yahoo Music #############\n"
     ]
    }
   ],
   "source": [
    "#########################################################################################\n",
    "'''In this section we train the models.\n",
    "   The user can control the following parameters:\n",
    "   num_epochs, bs, lr, patience, \n",
    "   choose_dataset, comments, parameters_arr.\n",
    "   The definition of each variable is noted next to it.'''\n",
    "\n",
    "base_path = os.getcwd()+'/'\n",
    "num_epochs = 200\n",
    "bs = 256 #batch size\n",
    "lr = 0.001 #learning rate\n",
    "patience = 4 # number of epochs in which the performance does not increase and therefore we stop the training\n",
    "choose_dataset = 3 # 1:ML100K, 2:ML1M, 3:Yahoo, 4:Amazon\n",
    "comments = 'test' # the comment added to the model's name at the end of the run.\n",
    "                  # The object save function: save_obj(models_dict, os.path.join(base_path + 'models/', f'models_dict_{ds_title}_{comments}'))\n",
    "                  # For example: models_dict_Movielens 100K_test.pkl.\n",
    "\n",
    "'''''''''\n",
    "parameters_arr contains the models' type and hyperparameters:\n",
    "\n",
    "first field - model type:  MF:1, AMP-X:X (X=[2-99]), SpectralCF:444, JCA:555, CDAE:666, ConvNcf:777, NCF:888\n",
    "second field - number of latent dimensions, e.g., 8, 16, 32 ...\n",
    "third field - alpha (a hyperparameter in the loss function)\n",
    "fourth field - positive lambda (a hyperparameter in the loss function)\n",
    "fifth field - negative lambda (a hyperparameter in the loss function)\n",
    "'''''''''\n",
    "\n",
    "parameters_arr = [(1,8,1,0,0), (3,64,0.5,0.005,0.005)] # In this example we run a MF with 8 latent dimensions\n",
    "                                                       # and AMP-3 with 64 latent dimensions, alpha=0.5, lambda_p=lambda_n=0.005\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "if choose_dataset == 1:\n",
    "    ds = 'ml-latest-small'\n",
    "    ds_title = 'Movielens 100K'\n",
    "    \n",
    "if choose_dataset == 2:\n",
    "    ds = 'ml-1m'\n",
    "    ds_title = 'Movielens 1M'\n",
    "\n",
    "if choose_dataset == 3:\n",
    "    ds = 'yahoo_music_data'\n",
    "    ds_title = 'Yahoo Music'\n",
    "    \n",
    "if choose_dataset == 4:\n",
    "    ds = 'amazon_Video_Games_data'\n",
    "    ds_title = 'Amazon Video Games'\n",
    "\n",
    "    \n",
    "'''Data Processing'''\n",
    "print(f'Using device: {device}')\n",
    "print(f'Training on {ds_title}')\n",
    "pos_samples_idx_dict, neg_samples_idx_dict, n, m, \\\n",
    "datasets, dataset_sizes, test_dict, userId_to_index, \\\n",
    "movieId_to_index, ratings, index_to_movieId, \\\n",
    "index_to_userId, movies = prep_trial(ds)\n",
    "print('\\n -------Data Prep Finished, Starting Training--------')\n",
    "print()\n",
    "\n",
    "models_dict = {}\n",
    "\n",
    "for model_key in parameters_arr:\n",
    "\n",
    "    num_personas, num_factors, alpha, lamda_pos, lamda_neg = model_key\n",
    "    rnd_seed = RND_SEED\n",
    "    random.seed(RND_SEED)\n",
    "    torch.manual_seed(RND_SEED)\n",
    "    np.random.seed(RND_SEED)\n",
    "\n",
    "    print('model type, #latent dimensions, alpha, positive lambda, negative lambda:', num_personas, num_factors, alpha, lamda_pos, lamda_neg)       \n",
    "    print()\n",
    "    \n",
    "    model = Model(num_factors, num_personas, pos_samples_idx_dict, neg_samples_idx_dict, n, m, datasets,\n",
    "                dataset_sizes, test_dict, userId_to_index, movieId_to_index, ratings, index_to_movieId,\n",
    "                index_to_userId, num_epochs = num_epochs, bs = bs, lr = lr, \n",
    "                alpha = alpha , lamda_pos = lamda_pos, lamda_neg = lamda_neg, patience = patience)\n",
    "\n",
    "    model.train_model()\n",
    "    model.get_top_k_movies_idxs_low_ram()\n",
    "    models_dict[model_key] = model\n",
    "    print('\\n ######################################################## \\n')\n",
    "\n",
    "save_obj(models_dict, os.path.join(base_path + 'models/', f'models_dict_{ds_title}_{comments}'))\n",
    "\n",
    "for key in models_dict.keys():\n",
    "    print(f'Model: {key}')\n",
    "    print(f'Best HR: {models_dict[key].best_hr:.4f} ||  Best NDCG: {models_dict[key].best_ndcg:.4f}')\n",
    "    print('\\n')\n",
    "print(f'############## Finished Training Dataset {ds_title} #############')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to get the HR@K and NDCG@K for different K's use: model.HR_and_NDCG_at_K(K)\n",
    "# The first number will be the HR@K and the second one the NDCG@K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Loading\n"
     ]
    }
   ],
   "source": [
    "'''In this section, you can load the trained models.\n",
    "   \n",
    "   Inputs: \n",
    "   dataset_name (one of the datasets given in the comment)\n",
    "   comment (as in the previous section)\n",
    "   \n",
    "   Output:\n",
    "   models_dict - a dictionary of the loaded models\n",
    "   '''\n",
    "\n",
    "dataset_name = 'Movielens 100K' #'Movielens 100K', 'Movielens 1M', 'Yahoo Music', 'Amazon Video Games\n",
    "comment = 'test'\n",
    "\n",
    "base_path = os.getcwd()+'/'\n",
    "models_dict = load_obj(os.path.join(base_path + 'models/', f'models_dict_{dataset_name}_{comment}'))\n",
    "\n",
    "print('Finish Loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "d61-vtVavznp",
    "qrStJudpv7D2",
    "bGIKEiPPv78I",
    "QqzZ6iXMv8eh",
    "_6B8c_Nev9Ac",
    "gwLDvX3QwniW",
    "zAUO6kF1wojj",
    "HAxDuNOW0PcJ",
    "4u1E2iOE0Pu3"
   ],
   "name": "AMP_CF_Final_1M_Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
